{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ProjectAthena**\n",
        "## **Subsection: Q-Net**\n",
        "### Q-Net: Optimizing the performance of off policy reinforcement learning algorithims using supervised meta learned networks as replacements.\n",
        "## **Goal:**\n",
        "#### - Outperform Q-Learning algorithims in training time and effectiveness\n",
        "#### - If possible, generate a replacement to the Q algorithim that is generalized to all programs\n",
        "#### - If possible, generate a model, so effective, that it replaces the existing RL algorithms that already outperform Q-Learning"
      ],
      "metadata": {
        "id": "XyzSvy9AJt-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Policy Gradient (aka Reinforce)**"
      ],
      "metadata": {
        "id": "s-gdDCYUOMqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports and Global Variables**"
      ],
      "metadata": {
        "id": "Clejc8YsEm1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cherry-rl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbyYvtFGHO1i",
        "outputId": "75551e0e-3e04-4411-b982-95b9bc2aaf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cherry-rl\n",
            "  Downloading cherry-rl-0.1.4.tar.gz (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from cherry-rl) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.7/dist-packages (from cherry-rl) (0.17.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cherry-rl) (1.12.0+cu113)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.9->cherry-rl) (1.7.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.9->cherry-rl) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.9->cherry-rl) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.10.9->cherry-rl) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->cherry-rl) (4.1.1)\n",
            "Building wheels for collected packages: cherry-rl\n",
            "  Building wheel for cherry-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cherry-rl: filename=cherry_rl-0.1.4-py3-none-any.whl size=57842 sha256=68b67780cff3f30d0ed56136518cd5ac273f5dcc2e70c492edf0c85ecfaffcf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/34/30/42ee307e7214d2af33c5999c6379feac44f1734baa02c2d850\n",
            "Successfully built cherry-rl\n",
            "Installing collected packages: cherry-rl\n",
            "Successfully installed cherry-rl-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cherry.distributions as distributions\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "from cherry.td import discount\n",
        "from cherry import normalize\n",
        "import torch.nn as nn\n",
        "import cherry as ch\n",
        "import gym.spaces\n",
        "import torch\n",
        "import gym"
      ],
      "metadata": {
        "id": "MwvOCF6IHCTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = \"CartPole-v0\"\n",
        "render = False\n",
        "gamma = 0.99\n",
        "\n",
        "# Only for getting timesteps, and obs-action spaces\n",
        "sample_env = gym.make(env_name)\n",
        "timestep_limit = sample_env.spec.max_episode_steps\n",
        "obs_space = sample_env.observation_space\n",
        "action_space = sample_env.action_space\n",
        "print(\"Observation space:\", obs_space)\n",
        "print(\"Action space:\", action_space)\n",
        "\n",
        "obs_size = obs_space.low.size\n",
        "action_size = action_space.n\n",
        "\n",
        "def update(replay, optimizer):\n",
        "    policy_loss = []\n",
        "    total_rewards = 0\n",
        "\n",
        "    # Discount and normalize rewards\n",
        "    rewards = ch.discount(gamma, replay.reward(), replay.done())\n",
        "    rewards = ch.normalize(rewards)\n",
        "\n",
        "    # Compute loss\n",
        "    for sars, reward in zip(replay, rewards):\n",
        "        log_prob = sars.log_prob\n",
        "        policy_loss.append(-log_prob * reward)\n",
        "        \n",
        "        total_rewards += reward\n",
        "\n",
        "    # Take optimization step\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return total_rewards\n",
        "\n",
        "def get_action_value(state, policy):\n",
        "    mass = Categorical(policy(state))\n",
        "    action = mass.sample()\n",
        "    info = {\n",
        "        'log_prob': mass.log_prob(action),  # Cache log_prob for later\n",
        "    }\n",
        "    return action, info"
      ],
      "metadata": {
        "id": "fel9O73cMHrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.network = torch.nn.Sequential(\n",
        "            nn.Linear(env.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, env.action_size),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "env = gym.make(env_name).env\n",
        "env = ch.envs.Torch(env)\n",
        "env = ch.envs.Runner(env)\n",
        "replay = ch.ExperienceReplay()\n",
        "device = \"cpu\"\n",
        "\n",
        "policy = PolicyNet(env)\n",
        "optimizer = torch.optim.RAdam(policy.parameters())\n",
        "get_action = lambda state: get_action_value(state, policy)\n",
        "epochs = 5\n",
        "\n",
        "for e in range(epochs):\n",
        "    replay = env.run(get_action, episodes=100, render=render)\n",
        "    print(f\"Epoch {e}  Total Reward: {round(update(replay, optimizer).item(), 2)}\\ttimesteps: {len(replay)}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "k66X8Z-8MIde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DQN**"
      ],
      "metadata": {
        "id": "bdvuWvgwOSS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports and Global Variables**"
      ],
      "metadata": {
        "id": "uyJR3OeOOSS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simple_rl"
      ],
      "metadata": {
        "id": "N3ovXn57Qk67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cherry.distributions as distributions\n",
        "from torch.distributions import Categorical\n",
        "from collections import defaultdict\n",
        "from prettytable import PrettyTable\n",
        "import torch.nn.functional as F\n",
        "from cherry.td import discount\n",
        "from cherry import normalize\n",
        "import torch.nn as nn\n",
        "import cherry as ch\n",
        "import numpy as np\n",
        "import gym.spaces\n",
        "import argparse\n",
        "import random\n",
        "import random\n",
        "import tqdm\n",
        "import torch\n",
        "import time\n",
        "import gym"
      ],
      "metadata": {
        "id": "ThkljFa2OSS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other imports.\n",
        "from simple_rl.agents.AgentClass import Agent\n",
        "\n",
        "class QNet(Agent):\n",
        "  def __init__(self, input, output):\n",
        "    self.model = \n",
        "\n",
        "class QLearningAgent(Agent):\n",
        "    ''' Implementation for a Q Learning Agent '''\n",
        "\n",
        "    def __init__(self, actions, name=\"Q-learning\", alpha=0.1, gamma=0.99, epsilon=0.1, explore=\"uniform\", anneal=False, custom_q_init=None, default_q=0):\n",
        "        '''\n",
        "        Args:\n",
        "            actions (list): Contains strings denoting the actions.\n",
        "            name (str): Denotes the name of the agent.\n",
        "            alpha (float): Learning rate.\n",
        "            gamma (float): Discount factor.\n",
        "            epsilon (float): Exploration term.\n",
        "            explore (str): One of {softmax, uniform}. Denotes explore policy.\n",
        "            custom_q_init (defaultdict{state, defaultdict{action, float}}): a dictionary of dictionaries storing the initial q-values. Can be used for potential shaping (Wiewiora, 2003)\n",
        "            default_q (float): the default value to initialize every entry in the q-table with [by default, set to 0.0]\n",
        "        '''\n",
        "        name_ext = \"-\" + explore if explore != \"uniform\" else \"\"\n",
        "        Agent.__init__(self, name=name + name_ext, actions=actions, gamma=gamma)\n",
        "\n",
        "        # Set/initialize parameters and other relevant classwide data\n",
        "        self.alpha, self.alpha_init = alpha, alpha\n",
        "        self.epsilon, self.epsilon_init = epsilon, epsilon\n",
        "        self.step_number = 0\n",
        "        self.anneal = anneal\n",
        "        self.default_q = default_q # 0 # 1 / (1 - self.gamma)\n",
        "        self.explore = explore\n",
        "        self.custom_q_init = custom_q_init\n",
        "        self.qnet = Qnet()\n",
        "        # Q Function:\n",
        "        if self.custom_q_init:\n",
        "            self.q_func = self.custom_q_init\n",
        "        else:\n",
        "            self.q_func = defaultdict(lambda: defaultdict(lambda: self.default_q))\n",
        "        \n",
        "        # Key: state\n",
        "        # Val: dict\n",
        "            #   Key: action\n",
        "            #   Val: q-value\n",
        "\n",
        "\n",
        "    def get_parameters(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            (dict) key=param_name (str) --> val=param_val (object).\n",
        "        '''\n",
        "        param_dict = defaultdict(int)\n",
        "\n",
        "        param_dict[\"alpha\"] = self.alpha\n",
        "        param_dict[\"gamma\"] = self.gamma\n",
        "        param_dict[\"epsilon\"] = self.epsilon_init\n",
        "        param_dict[\"anneal\"] = self.anneal\n",
        "        param_dict[\"explore\"] = self.explore\n",
        "\n",
        "        return param_dict\n",
        "\n",
        "    # --------------------------------\n",
        "    # ---- CENTRAL ACTION METHODS ----\n",
        "    # --------------------------------\n",
        "\n",
        "    def act(self, state, reward, learning=True):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "            reward (float)\n",
        "        Returns:\n",
        "            (str)\n",
        "        Summary:\n",
        "            The central method called during each time step.\n",
        "            Retrieves the action according to the current policy\n",
        "            and performs updates given (s=self.prev_state,\n",
        "            a=self.prev_action, r=reward, s'=state)\n",
        "        '''\n",
        "        if learning:\n",
        "            self.update(self.prev_state, self.prev_action, reward, state)\n",
        "        if self.explore == \"softmax\":\n",
        "            # Softmax exploration\n",
        "            action = self.soft_max_policy(state)\n",
        "        else:\n",
        "            # Uniform exploration\n",
        "            action = self.epsilon_greedy_q_policy(state)\n",
        "\n",
        "        self.prev_state = state\n",
        "        self.prev_action = action\n",
        "        self.step_number += 1\n",
        "\n",
        "        # Anneal params.\n",
        "        if learning and self.anneal:\n",
        "            self._anneal()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def epsilon_greedy_q_policy(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "        Returns:\n",
        "            (str): action.\n",
        "        '''\n",
        "        # Policy: Epsilon of the time explore, otherwise, greedyQ.\n",
        "        if numpy.random.random() > self.epsilon:\n",
        "            # Exploit.\n",
        "            action = self.get_max_q_action(state)\n",
        "        else:\n",
        "            # Explore\n",
        "            action = numpy.random.choice(self.actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def soft_max_policy(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State): Contains relevant state information.\n",
        "        Returns:\n",
        "            (str): action.\n",
        "        '''\n",
        "        return numpy.random.choice(self.actions, 1, p=self.get_action_distr(state))[0]\n",
        "\n",
        "    # ---------------------------------\n",
        "    # ---- Q VALUES AND PARAMETERS ----\n",
        "    # ---------------------------------\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "            action (str)\n",
        "            reward (float)\n",
        "            next_state (State)\n",
        "        Summary:\n",
        "            Updates the internal Q Function according to the Bellman Equation. (Classic Q Learning update)\n",
        "        '''\n",
        "        # If this is the first state, just return.\n",
        "        if state is None:\n",
        "            self.prev_state = next_state\n",
        "            return\n",
        "\n",
        "        # Update the Q Function.\n",
        "        max_q_curr_state = self.get_max_q_value(next_state)\n",
        "        prev_q_val = self.get_q_value(state, action)\n",
        "        self.q_func[state][action] = (1 - self.alpha) * prev_q_val + self.alpha * (reward + self.gamma*max_q_curr_state)\n",
        "        \n",
        "\n",
        "    def _anneal(self):\n",
        "        # Taken from \"Note on learning rate schedules for stochastic optimization, by Darken and Moody (Yale)\":\n",
        "        self.alpha = self.alpha_init / (1.0 +  (self.step_number / 1000.0)*(self.episode_number + 1) / 2000.0 )\n",
        "        self.epsilon = self.epsilon_init / (1.0 + (self.step_number / 1000.0)*(self.episode_number + 1) / 2000.0 )\n",
        "\n",
        "    def _compute_max_qval_action_pair(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "        Returns:\n",
        "            (tuple) --> (float, str): where the float is the Qval, str is the action.\n",
        "        '''\n",
        "        # Grab random initial action in case all equal\n",
        "        best_action = random.choice(self.actions)\n",
        "        max_q_val = float(\"-inf\")\n",
        "        shuffled_action_list = self.actions[:]\n",
        "        random.shuffle(shuffled_action_list)\n",
        "\n",
        "        # Find best action (action w/ current max predicted Q value)\n",
        "        for action in shuffled_action_list:\n",
        "            q_s_a = self.get_q_value(state, action)\n",
        "            if q_s_a > max_q_val:\n",
        "                max_q_val = q_s_a\n",
        "                best_action = action\n",
        "\n",
        "        return max_q_val, best_action\n",
        "\n",
        "    def get_max_q_action(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "        Returns:\n",
        "            (str): denoting the action with the max q value in the given @state.\n",
        "        '''\n",
        "        return self._compute_max_qval_action_pair(state)[1]\n",
        "\n",
        "    def get_max_q_value(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "        Returns:\n",
        "            (float): denoting the max q value in the given @state.\n",
        "        '''\n",
        "        return self._compute_max_qval_action_pair(state)[0]\n",
        "\n",
        "    def get_value(self, state):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "        Returns:\n",
        "            (float)\n",
        "        '''\n",
        "        return self.get_max_q_value(state)\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "            action (str)\n",
        "        Returns:\n",
        "            (float): denoting the q value of the (@state, @action) pair.\n",
        "        '''\n",
        "        return self.q_func[state][action]\n",
        "\n",
        "    def get_action_distr(self, state, beta=0.2):\n",
        "        '''\n",
        "        Args:\n",
        "            state (State)\n",
        "            beta (float): Softmax temperature parameter.\n",
        "        Returns:\n",
        "            (list of floats): The i-th float corresponds to the probability\n",
        "            mass associated with the i-th action (indexing into self.actions)\n",
        "        '''\n",
        "        all_q_vals = []\n",
        "        for i, action in enumerate(self.actions):\n",
        "            all_q_vals.append(self.get_q_value(state, action))\n",
        "\n",
        "        # Softmax distribution.\n",
        "        total = sum([numpy.exp(beta * qv) for qv in all_q_vals])\n",
        "        softmax = [numpy.exp(beta * qv) / total for qv in all_q_vals]\n",
        "\n",
        "        return softmax\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_number = 0\n",
        "        self.episode_number = 0\n",
        "        if self.custom_q_init:\n",
        "            self.q_func = self.custom_q_init\n",
        "        else:\n",
        "            self.q_func = defaultdict(lambda : defaultdict(lambda: self.default_q))\n",
        "        Agent.reset(self)\n",
        "\n",
        "    def end_of_episode(self):\n",
        "        '''\n",
        "        Summary:\n",
        "            Resets the agents prior pointers.\n",
        "        '''\n",
        "        if self.anneal:\n",
        "            self._anneal()\n",
        "        Agent.end_of_episode(self)\n",
        "\n",
        "    def print_v_func(self):\n",
        "        '''\n",
        "        Summary:\n",
        "            Prints the V function.\n",
        "        '''\n",
        "        for state in self.q_func.keys():\n",
        "            print(state, self.get_value(state))\n",
        "\n",
        "    def print_q_func(self):\n",
        "        '''\n",
        "        Summary:\n",
        "            Prints the Q function.\n",
        "        '''\n",
        "        if len(self.q_func) == 0:\n",
        "            print(\"Q Func empty!\")\n",
        "        else:\n",
        "            for state, actiond in self.q_func.items():\n",
        "                print(state)\n",
        "                for action, q_val in actiond.items():\n",
        "                    print(\"    \", action, q_val)"
      ],
      "metadata": {
        "id": "P6juUij1QZzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}